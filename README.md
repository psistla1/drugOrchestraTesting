# Drug Orchestra Testing

Drug Orchestra is a multi-learning machine learning algorithm that combines several datasets to merge the training and prediction of drug targets, response, and side effects.  For this model, more testing is required to paint a better view of drug orchestra's reliability with other algorithms and further testing on the intractability of the dataset. Algorithms have already been applied to predict each dataset, as well as on their intractability both as datasets and as whole tasks. These graphs will be used as a starting point to refer back to as I develop more algorithms for use and comparison. I have created several classification and regression algorithms for evaluating the performance of the data, along with accuracy and other goodness metrics in the form of a confusion matrix. The charts referred to before were used as a template to build upon or attempt to recreate with different algorithms. My results were relatively as expected and in-line with the prior charts, showing improvement gained from the use of Drug Orchestra as a complex model. There were outliers however, and parts of data that led to incorrect conclusions due to problems with data feature extraction and dimensionality reduction. The large amount of data, approximately 10.2 GB, created several issues in the prepossessing and processing of the data. To fix this, several techniques were used in tandem to make the data manageable, which made processing feasible but resulted in some information loss.

Original Article: https://www.biorxiv.org/content/10.1101/2020.11.17.385757v1.full

## Introduction

When it comes to the complex prediction of how someone's genes affect drug targets, responses, and side effects, not many advancements have been made. Different models have individually been used on various datasets to test their accuracy, but no progress has been made in bridging the gap between these three areas. There is potential to transfer data between the types of predictive models to create more powerful algorithms through the cooperation and communication between these datasets.

Drug Orchestra is a proposed solution by Yuepeng Jiang1 that aims to combine several unique complex machine learning algorithms for drug testing, responses, and side effects into one powerful model. Their work is publicly available on GitHub and will serve as the foundation for my work to reaffirm the evidence of possible transferability between these drug-related tasks.

Specifically, three charts will be used to confirm the connections between these tasks. The first chart measures the performance of machine learning algorithms in comparison to Drug Orchestra, aiming to demonstrate that combining all tasks and datasets can enhance data transferability and benefit prediction accuracy. The next chart will illustrate the performance gains from training and testing different datasets on each other. The final chart will assess the performance gains between the different groups as a whole, aiming to establish a direct link between their transferability.

My results generally agree with those provided by Drug Orchestra. Some algorithms were found to be more effective than Drug Orchestra, although there were some biases that might have affected the results. The datasets and tasks also seem to support the original findings of a link between datasets and tasks, with transferability being a viable option.

Some challenges encountered included the large volumes of data that required extensive preprocessing. To manage this, we employed PCA and feature extraction techniques. A significant drawback of this approach was the loss of information due to dimensionality reduction, which was exacerbated when we changed the data type from 64-bit integers to 32-bit, saving substantial amounts of space and processing power but at the cost of reducing the fidelity of the original dataset.

## Process of Use
For the Exploratory Data Analysis, use the file EDA_DO(1) to calculate the shape of the data, any missing values, and create correlation matrixes and simplified PCA models on each dataset. 

Several methods of pre-processing must be used to shorten the 10.2 gb of data we need to repeatedly work on. This is found int DrugOrchestra_PCA(1). To do this we need to first seperate the class variable of the last column away from the data so that it doesn't get altered from the pca. Then the features must be changed. Initially being a 64-bit integer, it has to be converted to a 32-bit integer which does result in some information and data lost. The PCA will be done with 50 components, which will reduce it's dimensionality well while returning the information needed to get accurate results during the analysis period after this is complete. The PCA data is what will be used from now on.

For the first chart, we need to take all the classification algorithms possible. Look at DrugOrchestra_Classification. All the algorithms we learned over the course of our semester are being employed to train and predict each dataset individually. Taking all the files from the folder after being pre-processed, we individually value it by each algorithm, determining the AUROC and AURPRC for the classifiers. For Regression however, using DrugOrchestra_Regression, we use the mean squared error and Spearman's Correlation Coefficient.   For the non-continuous data of drug targets and side effects, AUROC measures the ability of a classifier to distinguish between classes across all thresholds, plotting true positive rates against false positive rates. It ranges from 0 to 1, with higher values indicating better classification performance. AUPRC focuses on the performance of a classifier's precision and recall, particularly valuable in contexts with imbalanced classes. It also ranges from 0 to 1, with higher values reflecting better performance on the minority class. For the continuous data of drug response, MSE is used to quantify the average squared difference between the actual values and the values predicted by a model while SCC measures the strength and direction of the monotonic relationship between two ranked variables. These are the metrics we are measuring, along with other goodness metrics like accuracy, confusion matrixes, etc. This data is shown in the github as an output log.

  For the second chart, we need to calculate the performance gain of the initial chart based on transferability between individual datasets. To do this, in Chart2_DO and Chart2_DO_CtR, we train the machine learning model using one dataset, then test it on our desired dataset. The output of this data will be MSE or AUROC depending on whether the test dataset is continuous or not. Then we compare it to the data in chart 1 using a performance gain equaion. (Performance Gain = \frac{P_{a}-P_{b}}{P_{b}} * 100\%\)

   For the third chart, we need to combine the datasets we have as three different groups into three combined datasets. This is possible after preprocessing in CombineTasks. The same process we used for both Chart 1 and Chart 2 is now employed, as we calculate the Classification or Regression algorithm for AUROC or MSE, then use the Chart 2 algorithm to compare these graphs against each other and determine the performance gain using the same equation to determine the performance gain.
